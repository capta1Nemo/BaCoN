
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all False
c_0 ['lcdm']
c_1 ['dgp', 'fR', 'rand', 'wcdm']
dataset_balanced False
include_last False
log_path 
restore False
fname 5_small
model_name custom
my_path None
DIR data/test_data/
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
normalization stdcosmo
sample_pace 4
k_max 2.5
i_max None
add_noise True
n_noisy_samples 10
add_shot True
add_sys True
sigma_sys 5.0
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 2500
patience 100
GPU True
decay 0.95
BatchNorm True

------------ CREATING DATA GENERATORS ------------
labels : ['dgp', 'fR', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fR': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 2500 training examples
fR - 2500 training examples
lcdm - 2500 training examples
rand - 2500 training examples
wcdm - 2500 training examples

N. of data files: 2500
get_all_indexes labels dict: {'dgp': 0, 'fR': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels: 5
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 2125
N of files in validation set: 375
N of files in test set: 0
Check - total: 2500
--create_generators, train indexes
batch_size: 2500
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 2100
--create_generators, validation indexes
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 350
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 2100, 2500, 5, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
list_IDs length: 2100
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 42
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 42

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
list_IDs length: 350
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 7
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 7
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
using 1D layers and 4 channels
Expected output dimension of layer conv1d_flipout: 46.0
Expected output dimension of layer max_pooling1d: 23.0
Expected output dimension of layer conv1d_flipout_1: 10.0
Expected output dimension of layer max_pooling1d_1: 9.0
Expected output dimension of layer conv1d_flipout_2: 8.0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlipo  (None, 46, 8)            648       
 ut)                                                             
                                                                 
 max_pooling1d (MaxPooling1D  (None, 23, 8)            0         
 )                                                               
                                                                 
 batch_normalization (BatchN  (None, 23, 8)            32        
 ormalization)                                                   
                                                                 
 conv1d_flipout_1 (Conv1DFli  (None, 10, 16)           1296      
 pout)                                                           
                                                                 
 max_pooling1d_1 (MaxPooling  (None, 9, 16)            0         
 1D)                                                             
                                                                 
 batch_normalization_1 (Batc  (None, 9, 16)            64        
 hNormalization)                                                 
                                                                 
 conv1d_flipout_2 (Conv1DFli  (None, 8, 32)            2080      
 pout)                                                           
                                                                 
 batch_normalization_2 (Batc  (None, 8, 32)            128       
 hNormalization)                                                 
                                                                 
 global_average_pooling1d (G  (None, 32)               0         
 lobalAveragePooling1D)                                          
                                                                 
 dense_flipout (DenseFlipout  (None, 32)               2080      
 )                                                               
                                                                 
 batch_normalization_3 (Batc  (None, 32)               128       
 hNormalization)                                                 
                                                                 
 dense_flipout_1 (DenseFlipo  (None, 5)                325       
 ut)                                                             
                                                                 
=================================================================
Total params: 6,781
Trainable params: 6,605
Non-trainable params: 176
_________________________________________________________________
None
GPU device not found ! Device: 
------------ TRAINING ------------

Features shape: (2500, 100, 4)
Labels shape: (2500, 5)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/5_small/tf_ckpts/ckpt-1
Time:  36.98s, ---- Loss: 0.5969, Acc.: 0.6425, Val. Loss: 3.2930, Val. Acc.: 0.2000

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/5_small/tf_ckpts/ckpt-2
Time:  28.08s, ---- Loss: 0.4126, Acc.: 0.8210, Val. Loss: 2.5346, Val. Acc.: 0.2274

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/5_small/tf_ckpts/ckpt-3
Time:  31.09s, ---- Loss: 0.3970, Acc.: 0.8637, Val. Loss: 1.4039, Val. Acc.: 0.6234

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/5_small/tf_ckpts/ckpt-4
Time:  32.82s, ---- Loss: 0.3731, Acc.: 0.8753, Val. Loss: 1.0103, Val. Acc.: 0.7415

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/5_small/tf_ckpts/ckpt-5
Time:  30.97s, ---- Loss: 0.3588, Acc.: 0.8818, Val. Loss: 0.9242, Val. Acc.: 0.7831

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/5_small/tf_ckpts/ckpt-6
Time:  31.91s, ---- Loss: 0.3551, Acc.: 0.8884, Val. Loss: 0.7904, Val. Acc.: 0.8582

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/5_small/tf_ckpts/ckpt-7
Time:  31.47s, ---- Loss: 0.3572, Acc.: 0.8940, Val. Loss: 0.7352, Val. Acc.: 0.8896

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/5_small/tf_ckpts/ckpt-8
Time:  30.30s, ---- Loss: 0.3364, Acc.: 0.8978, Val. Loss: 0.7130, Val. Acc.: 0.8977

Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/5_small/tf_ckpts/ckpt-9
Time:  32.02s, ---- Loss: 0.3506, Acc.: 0.9019, Val. Loss: 0.6971, Val. Acc.: 0.9027

Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/5_small/tf_ckpts/ckpt-10
Time:  34.71s, ---- Loss: 0.3458, Acc.: 0.9056, Val. Loss: 0.6753, Val. Acc.: 0.9139

Epoch 10
Loss did not decrease. Count = 1
Time:  32.74s, ---- Loss: 0.3339, Acc.: 0.9087, Val. Loss: 0.6771, Val. Acc.: 0.9151

Epoch 11
Loss did not decrease. Count = 2
Time:  32.19s, ---- Loss: 0.3454, Acc.: 0.9116, Val. Loss: 0.6953, Val. Acc.: 0.8993

Epoch 12
Loss did not decrease. Count = 3
Time:  31.49s, ---- Loss: 0.3151, Acc.: 0.9130, Val. Loss: 0.6780, Val. Acc.: 0.9069

Epoch 13
Loss did not decrease. Count = 4
Time:  33.01s, ---- Loss: 0.3125, Acc.: 0.9151, Val. Loss: 0.6754, Val. Acc.: 0.8985

Epoch 14
Validation loss decreased. Saved checkpoint for step 15: models/5_small/tf_ckpts/ckpt-11
Time:  28.99s, ---- Loss: 0.3076, Acc.: 0.9162, Val. Loss: 0.6408, Val. Acc.: 0.9175

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/5_small/tf_ckpts/ckpt-12
Time:  29.53s, ---- Loss: 0.3028, Acc.: 0.9183, Val. Loss: 0.6379, Val. Acc.: 0.9197

Epoch 16
Loss did not decrease. Count = 1
Time:  29.84s, ---- Loss: 0.3106, Acc.: 0.9214, Val. Loss: 0.6431, Val. Acc.: 0.9139

Epoch 17
Loss did not decrease. Count = 2
Time:  33.32s, ---- Loss: 0.2995, Acc.: 0.9212, Val. Loss: 0.6514, Val. Acc.: 0.9077

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/5_small/tf_ckpts/ckpt-13
Time:  31.63s, ---- Loss: 0.3030, Acc.: 0.9240, Val. Loss: 0.6276, Val. Acc.: 0.9219

Epoch 19
Loss did not decrease. Count = 1
Time:  29.70s, ---- Loss: 0.2982, Acc.: 0.9234, Val. Loss: 0.6570, Val. Acc.: 0.9016

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/5_small/tf_ckpts/ckpt-14
Time:  31.62s, ---- Loss: 0.2952, Acc.: 0.9230, Val. Loss: 0.6183, Val. Acc.: 0.9246

Epoch 21
Loss did not decrease. Count = 1
Time:  31.13s, ---- Loss: 0.2952, Acc.: 0.9252, Val. Loss: 0.6222, Val. Acc.: 0.9205

Epoch 22
Loss did not decrease. Count = 2
Time:  32.14s, ---- Loss: 0.2876, Acc.: 0.9263, Val. Loss: 0.6210, Val. Acc.: 0.9210

Epoch 23
Loss did not decrease. Count = 3
Time:  30.85s, ---- Loss: 0.2965, Acc.: 0.9271, Val. Loss: 0.6194, Val. Acc.: 0.9209

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/5_small/tf_ckpts/ckpt-15
Time:  32.76s, ---- Loss: 0.2919, Acc.: 0.9266, Val. Loss: 0.6144, Val. Acc.: 0.9222

Epoch 25
Validation loss decreased. Saved checkpoint for step 26: models/5_small/tf_ckpts/ckpt-16
Time:  29.10s, ---- Loss: 0.2794, Acc.: 0.9287, Val. Loss: 0.6059, Val. Acc.: 0.9271

Epoch 26
Loss did not decrease. Count = 1
Time:  31.72s, ---- Loss: 0.2935, Acc.: 0.9289, Val. Loss: 0.6157, Val. Acc.: 0.9215

Epoch 27
Loss did not decrease. Count = 2
Time:  31.65s, ---- Loss: 0.2913, Acc.: 0.9283, Val. Loss: 0.6327, Val. Acc.: 0.9071

Epoch 28
Loss did not decrease. Count = 3
Time:  30.76s, ---- Loss: 0.2960, Acc.: 0.9268, Val. Loss: 0.6281, Val. Acc.: 0.9129

Epoch 29
Loss did not decrease. Count = 4
Time:  29.19s, ---- Loss: 0.2891, Acc.: 0.9274, Val. Loss: 0.6254, Val. Acc.: 0.9116

Epoch 30
Loss did not decrease. Count = 5
Time:  28.09s, ---- Loss: 0.2987, Acc.: 0.9282, Val. Loss: 0.6171, Val. Acc.: 0.9151

Epoch 31
Loss did not decrease. Count = 6
Time:  29.88s, ---- Loss: 0.2772, Acc.: 0.9288, Val. Loss: 0.6127, Val. Acc.: 0.9161

Epoch 32
Loss did not decrease. Count = 7
Time:  30.78s, ---- Loss: 0.2865, Acc.: 0.9285, Val. Loss: 0.6100, Val. Acc.: 0.9194

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/5_small/tf_ckpts/ckpt-17
Time:  30.34s, ---- Loss: 0.2764, Acc.: 0.9286, Val. Loss: 0.6019, Val. Acc.: 0.9221

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/5_small/tf_ckpts/ckpt-18
Time:  28.52s, ---- Loss: 0.2666, Acc.: 0.9306, Val. Loss: 0.5942, Val. Acc.: 0.9270

Epoch 35
Validation loss decreased. Saved checkpoint for step 36: models/5_small/tf_ckpts/ckpt-19
Time:  28.60s, ---- Loss: 0.2609, Acc.: 0.9313, Val. Loss: 0.5923, Val. Acc.: 0.9272

Epoch 36
Loss did not decrease. Count = 1
Time:  27.92s, ---- Loss: 0.2687, Acc.: 0.9314, Val. Loss: 0.5957, Val. Acc.: 0.9262

Epoch 37
Validation loss decreased. Saved checkpoint for step 38: models/5_small/tf_ckpts/ckpt-20
Time:  28.82s, ---- Loss: 0.2789, Acc.: 0.9313, Val. Loss: 0.5886, Val. Acc.: 0.9301

Epoch 38
Validation loss decreased. Saved checkpoint for step 39: models/5_small/tf_ckpts/ckpt-21
Time:  28.15s, ---- Loss: 0.2625, Acc.: 0.9319, Val. Loss: 0.5822, Val. Acc.: 0.9327

Epoch 39
Loss did not decrease. Count = 1
Time:  29.73s, ---- Loss: 0.2669, Acc.: 0.9325, Val. Loss: 0.5849, Val. Acc.: 0.9307

Epoch 40
Validation loss decreased. Saved checkpoint for step 41: models/5_small/tf_ckpts/ckpt-22
Time:  30.64s, ---- Loss: 0.2538, Acc.: 0.9330, Val. Loss: 0.5782, Val. Acc.: 0.9331

Epoch 41
Loss did not decrease. Count = 1
Time:  26.17s, ---- Loss: 0.2540, Acc.: 0.9333, Val. Loss: 0.5787, Val. Acc.: 0.9334

Epoch 42
Validation loss decreased. Saved checkpoint for step 43: models/5_small/tf_ckpts/ckpt-23
Time:  26.00s, ---- Loss: 0.2520, Acc.: 0.9334, Val. Loss: 0.5765, Val. Acc.: 0.9347

Epoch 43
Validation loss decreased. Saved checkpoint for step 44: models/5_small/tf_ckpts/ckpt-24
Time:  25.57s, ---- Loss: 0.2563, Acc.: 0.9334, Val. Loss: 0.5754, Val. Acc.: 0.9357

Epoch 44
Loss did not decrease. Count = 1
Time:  26.76s, ---- Loss: 0.2472, Acc.: 0.9332, Val. Loss: 0.5763, Val. Acc.: 0.9338

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/5_small/tf_ckpts/ckpt-25
Time:  26.01s, ---- Loss: 0.2442, Acc.: 0.9340, Val. Loss: 0.5720, Val. Acc.: 0.9359

Epoch 46
Validation loss decreased. Saved checkpoint for step 47: models/5_small/tf_ckpts/ckpt-26
Time:  28.17s, ---- Loss: 0.2527, Acc.: 0.9340, Val. Loss: 0.5684, Val. Acc.: 0.9377

Epoch 47
Loss did not decrease. Count = 1
Time:  25.95s, ---- Loss: 0.2539, Acc.: 0.9343, Val. Loss: 0.5729, Val. Acc.: 0.9357

Epoch 48
Loss did not decrease. Count = 2
Time:  26.97s, ---- Loss: 0.2593, Acc.: 0.9343, Val. Loss: 0.5711, Val. Acc.: 0.9352

Epoch 49
Loss did not decrease. Count = 3
Time:  27.11s, ---- Loss: 0.2491, Acc.: 0.9344, Val. Loss: 0.5700, Val. Acc.: 0.9361

Saving at models/5_small/hist.png
