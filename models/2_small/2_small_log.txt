
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune False
one_vs_all True
c_0 ['lcdm']
c_1 ['dgp', 'fR', 'rand', 'wcdm']
dataset_balanced False
include_last False
log_path 
restore False
fname 2_small
model_name custom
my_path None
DIR data/test_data/
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
normalization stdcosmo
sample_pace 4
k_max 2.5
i_max None
add_noise True
n_noisy_samples 10
add_shot True
add_sys True
sigma_sys 5.0
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable False
unfreeze False
lr 0.01
drop 0.5
n_epochs 50
val_size 0.15
test_size 0.0
batch_size 2500
patience 100
GPU True
decay 0.95
BatchNorm True
group_lab_dict {'dgp': 'non_lcdm', 'fR': 'non_lcdm', 'rand': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}

------------ CREATING DATA GENERATORS ------------
labels : ['lcdm', 'non_lcdm']
Labels encoding: 
{'lcdm': 0, 'non_lcdm': 1}
n_labels : 2
dgp - 2500 training examples
lcdm - 2500 training examples
rand - 2500 training examples
wcdm - 2500 training examples
fR - 2500 training examples

N. of data files: 2500
get_all_indexes labels dict: {'lcdm': 0, 'non_lcdm': 1}
create_generators n_labels: 2
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 2125
N of files in validation set: 375
N of files in test set: 0
Check - total: 2500
--create_generators, train indexes
batch_size: 2500
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 2100
--create_generators, validation indexes
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 350
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 2100, 2500, 5, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: True
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 2
list_IDs length: 2100
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 42
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 42

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: True
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 2
list_IDs length: 350
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 7
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 7
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
using 1D layers and 4 channels
Expected output dimension of layer conv1d_flipout: 46.0
Expected output dimension of layer max_pooling1d: 23.0
Expected output dimension of layer conv1d_flipout_1: 10.0
Expected output dimension of layer max_pooling1d_1: 9.0
Expected output dimension of layer conv1d_flipout_2: 8.0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlipo  (None, 46, 8)            648       
 ut)                                                             
                                                                 
 max_pooling1d (MaxPooling1D  (None, 23, 8)            0         
 )                                                               
                                                                 
 batch_normalization (BatchN  (None, 23, 8)            32        
 ormalization)                                                   
                                                                 
 conv1d_flipout_1 (Conv1DFli  (None, 10, 16)           1296      
 pout)                                                           
                                                                 
 max_pooling1d_1 (MaxPooling  (None, 9, 16)            0         
 1D)                                                             
                                                                 
 batch_normalization_1 (Batc  (None, 9, 16)            64        
 hNormalization)                                                 
                                                                 
 conv1d_flipout_2 (Conv1DFli  (None, 8, 32)            2080      
 pout)                                                           
                                                                 
 batch_normalization_2 (Batc  (None, 8, 32)            128       
 hNormalization)                                                 
                                                                 
 global_average_pooling1d (G  (None, 32)               0         
 lobalAveragePooling1D)                                          
                                                                 
 dense_flipout (DenseFlipout  (None, 32)               2080      
 )                                                               
                                                                 
 batch_normalization_3 (Batc  (None, 32)               128       
 hNormalization)                                                 
                                                                 
 dense_flipout_1 (DenseFlipo  (None, 2)                130       
 ut)                                                             
                                                                 
=================================================================
Total params: 6,586
Trainable params: 6,410
Non-trainable params: 176
_________________________________________________________________
None
GPU device not found ! Device: 
------------ TRAINING ------------

Features shape: (2500, 100, 4)
Labels shape: (2500, 2)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/2_small/tf_ckpts/ckpt-1
Time:  33.73s, ---- Loss: 0.3838, Acc.: 0.7523, Val. Loss: 1.2720, Val. Acc.: 0.8000

Epoch 1
Loss did not decrease. Count = 1
Time:  29.88s, ---- Loss: 0.3688, Acc.: 0.8705, Val. Loss: 1.5825, Val. Acc.: 0.8000

Epoch 2
Loss did not decrease. Count = 2
Time:  30.53s, ---- Loss: 0.3615, Acc.: 0.8878, Val. Loss: 1.4474, Val. Acc.: 0.8000

Epoch 3
Loss did not decrease. Count = 3
Time:  29.34s, ---- Loss: 0.3448, Acc.: 0.8979, Val. Loss: 1.2954, Val. Acc.: 0.8000

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/2_small/tf_ckpts/ckpt-2
Time:  30.08s, ---- Loss: 0.3006, Acc.: 0.9049, Val. Loss: 0.8996, Val. Acc.: 0.8050

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/2_small/tf_ckpts/ckpt-3
Time:  29.26s, ---- Loss: 0.2905, Acc.: 0.9123, Val. Loss: 0.7612, Val. Acc.: 0.8239

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/2_small/tf_ckpts/ckpt-4
Time:  30.27s, ---- Loss: 0.2824, Acc.: 0.9145, Val. Loss: 0.6566, Val. Acc.: 0.8620

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/2_small/tf_ckpts/ckpt-5
Time:  28.28s, ---- Loss: 0.2677, Acc.: 0.9182, Val. Loss: 0.6143, Val. Acc.: 0.8780

Epoch 8
Validation loss decreased. Saved checkpoint for step 9: models/2_small/tf_ckpts/ckpt-6
Time:  27.77s, ---- Loss: 0.2825, Acc.: 0.9230, Val. Loss: 0.5798, Val. Acc.: 0.8980

Epoch 9
Validation loss decreased. Saved checkpoint for step 10: models/2_small/tf_ckpts/ckpt-7
Time:  27.46s, ---- Loss: 0.2594, Acc.: 0.9236, Val. Loss: 0.5434, Val. Acc.: 0.9169

Epoch 10
Validation loss decreased. Saved checkpoint for step 11: models/2_small/tf_ckpts/ckpt-8
Time:  28.78s, ---- Loss: 0.2514, Acc.: 0.9252, Val. Loss: 0.5411, Val. Acc.: 0.9172

Epoch 11
Validation loss decreased. Saved checkpoint for step 12: models/2_small/tf_ckpts/ckpt-9
Time:  28.37s, ---- Loss: 0.2488, Acc.: 0.9274, Val. Loss: 0.5348, Val. Acc.: 0.9191

Epoch 12
Validation loss decreased. Saved checkpoint for step 13: models/2_small/tf_ckpts/ckpt-10
Time:  28.36s, ---- Loss: 0.2460, Acc.: 0.9288, Val. Loss: 0.5077, Val. Acc.: 0.9323

Epoch 13
Loss did not decrease. Count = 1
Time:  30.14s, ---- Loss: 0.2381, Acc.: 0.9307, Val. Loss: 0.5171, Val. Acc.: 0.9248

Epoch 14
Loss did not decrease. Count = 2
Time:  27.59s, ---- Loss: 0.2369, Acc.: 0.9317, Val. Loss: 0.5155, Val. Acc.: 0.9215

Epoch 15
Validation loss decreased. Saved checkpoint for step 16: models/2_small/tf_ckpts/ckpt-11
Time:  25.78s, ---- Loss: 0.2288, Acc.: 0.9312, Val. Loss: 0.5034, Val. Acc.: 0.9273

Epoch 16
Validation loss decreased. Saved checkpoint for step 17: models/2_small/tf_ckpts/ckpt-12
Time:  27.59s, ---- Loss: 0.2390, Acc.: 0.9325, Val. Loss: 0.4997, Val. Acc.: 0.9275

Epoch 17
Validation loss decreased. Saved checkpoint for step 18: models/2_small/tf_ckpts/ckpt-13
Time:  26.32s, ---- Loss: 0.2346, Acc.: 0.9336, Val. Loss: 0.4956, Val. Acc.: 0.9273

Epoch 18
Validation loss decreased. Saved checkpoint for step 19: models/2_small/tf_ckpts/ckpt-14
Time:  26.77s, ---- Loss: 0.2277, Acc.: 0.9344, Val. Loss: 0.4895, Val. Acc.: 0.9294

Epoch 19
Validation loss decreased. Saved checkpoint for step 20: models/2_small/tf_ckpts/ckpt-15
Time:  26.86s, ---- Loss: 0.2266, Acc.: 0.9352, Val. Loss: 0.4805, Val. Acc.: 0.9348

Epoch 20
Validation loss decreased. Saved checkpoint for step 21: models/2_small/tf_ckpts/ckpt-16
Time:  27.64s, ---- Loss: 0.2362, Acc.: 0.9360, Val. Loss: 0.4797, Val. Acc.: 0.9317

Epoch 21
Validation loss decreased. Saved checkpoint for step 22: models/2_small/tf_ckpts/ckpt-17
Time:  27.32s, ---- Loss: 0.2180, Acc.: 0.9356, Val. Loss: 0.4699, Val. Acc.: 0.9353

Epoch 22
Validation loss decreased. Saved checkpoint for step 23: models/2_small/tf_ckpts/ckpt-18
Time:  27.54s, ---- Loss: 0.2261, Acc.: 0.9369, Val. Loss: 0.4658, Val. Acc.: 0.9370

Epoch 23
Validation loss decreased. Saved checkpoint for step 24: models/2_small/tf_ckpts/ckpt-19
Time:  27.48s, ---- Loss: 0.2196, Acc.: 0.9363, Val. Loss: 0.4597, Val. Acc.: 0.9373

Epoch 24
Validation loss decreased. Saved checkpoint for step 25: models/2_small/tf_ckpts/ckpt-20
Time:  29.11s, ---- Loss: 0.2268, Acc.: 0.9370, Val. Loss: 0.4511, Val. Acc.: 0.9410

Epoch 25
Loss did not decrease. Count = 1
Time:  28.07s, ---- Loss: 0.2229, Acc.: 0.9372, Val. Loss: 0.4534, Val. Acc.: 0.9398

Epoch 26
Validation loss decreased. Saved checkpoint for step 27: models/2_small/tf_ckpts/ckpt-21
Time:  26.22s, ---- Loss: 0.2312, Acc.: 0.9370, Val. Loss: 0.4484, Val. Acc.: 0.9397

Epoch 27
Validation loss decreased. Saved checkpoint for step 28: models/2_small/tf_ckpts/ckpt-22
Time:  27.69s, ---- Loss: 0.2257, Acc.: 0.9376, Val. Loss: 0.4431, Val. Acc.: 0.9413

Epoch 28
Loss did not decrease. Count = 1
Time:  26.80s, ---- Loss: 0.2150, Acc.: 0.9388, Val. Loss: 0.4464, Val. Acc.: 0.9391

Epoch 29
Validation loss decreased. Saved checkpoint for step 30: models/2_small/tf_ckpts/ckpt-23
Time:  26.59s, ---- Loss: 0.2249, Acc.: 0.9383, Val. Loss: 0.4416, Val. Acc.: 0.9413

Epoch 30
Validation loss decreased. Saved checkpoint for step 31: models/2_small/tf_ckpts/ckpt-24
Time:  28.13s, ---- Loss: 0.2100, Acc.: 0.9387, Val. Loss: 0.4388, Val. Acc.: 0.9402

Epoch 31
Validation loss decreased. Saved checkpoint for step 32: models/2_small/tf_ckpts/ckpt-25
Time:  27.35s, ---- Loss: 0.2149, Acc.: 0.9394, Val. Loss: 0.4385, Val. Acc.: 0.9416

Epoch 32
Validation loss decreased. Saved checkpoint for step 33: models/2_small/tf_ckpts/ckpt-26
Time:  26.47s, ---- Loss: 0.2222, Acc.: 0.9395, Val. Loss: 0.4367, Val. Acc.: 0.9402

Epoch 33
Validation loss decreased. Saved checkpoint for step 34: models/2_small/tf_ckpts/ckpt-27
Time:  26.67s, ---- Loss: 0.2114, Acc.: 0.9402, Val. Loss: 0.4345, Val. Acc.: 0.9423

Epoch 34
Validation loss decreased. Saved checkpoint for step 35: models/2_small/tf_ckpts/ckpt-28
Time:  27.01s, ---- Loss: 0.2068, Acc.: 0.9399, Val. Loss: 0.4330, Val. Acc.: 0.9414

Epoch 35
Loss did not decrease. Count = 1
Time:  26.49s, ---- Loss: 0.2158, Acc.: 0.9402, Val. Loss: 0.4377, Val. Acc.: 0.9389

Epoch 36
Loss did not decrease. Count = 2
Time:  27.34s, ---- Loss: 0.2041, Acc.: 0.9396, Val. Loss: 0.4377, Val. Acc.: 0.9378

Epoch 37
Loss did not decrease. Count = 3
Time:  28.29s, ---- Loss: 0.2191, Acc.: 0.9400, Val. Loss: 0.4430, Val. Acc.: 0.9327

Epoch 38
Loss did not decrease. Count = 4
Time:  27.22s, ---- Loss: 0.2005, Acc.: 0.9406, Val. Loss: 0.4339, Val. Acc.: 0.9383

Epoch 39
Loss did not decrease. Count = 5
Time:  26.76s, ---- Loss: 0.2128, Acc.: 0.9415, Val. Loss: 0.4345, Val. Acc.: 0.9373

Epoch 40
Loss did not decrease. Count = 6
Time:  25.51s, ---- Loss: 0.2036, Acc.: 0.9408, Val. Loss: 0.4338, Val. Acc.: 0.9378

Epoch 41
Loss did not decrease. Count = 7
Time:  27.86s, ---- Loss: 0.2028, Acc.: 0.9403, Val. Loss: 0.4374, Val. Acc.: 0.9355

Epoch 42
Loss did not decrease. Count = 8
Time:  27.78s, ---- Loss: 0.2083, Acc.: 0.9407, Val. Loss: 0.4331, Val. Acc.: 0.9362

Epoch 43
Loss did not decrease. Count = 9
Time:  27.81s, ---- Loss: 0.2050, Acc.: 0.9415, Val. Loss: 0.4334, Val. Acc.: 0.9355

Epoch 44
Loss did not decrease. Count = 10
Time:  28.36s, ---- Loss: 0.2052, Acc.: 0.9412, Val. Loss: 0.4370, Val. Acc.: 0.9338

Epoch 45
Validation loss decreased. Saved checkpoint for step 46: models/2_small/tf_ckpts/ckpt-29
Time:  27.04s, ---- Loss: 0.2057, Acc.: 0.9410, Val. Loss: 0.4311, Val. Acc.: 0.9367

Epoch 46
Validation loss decreased. Saved checkpoint for step 47: models/2_small/tf_ckpts/ckpt-30
Time:  27.14s, ---- Loss: 0.2055, Acc.: 0.9412, Val. Loss: 0.4262, Val. Acc.: 0.9390

Epoch 47
Validation loss decreased. Saved checkpoint for step 48: models/2_small/tf_ckpts/ckpt-31
Time:  27.33s, ---- Loss: 0.2080, Acc.: 0.9412, Val. Loss: 0.4255, Val. Acc.: 0.9391

Epoch 48
Validation loss decreased. Saved checkpoint for step 49: models/2_small/tf_ckpts/ckpt-32
Time:  27.31s, ---- Loss: 0.2074, Acc.: 0.9416, Val. Loss: 0.4211, Val. Acc.: 0.9417

Epoch 49
Loss did not decrease. Count = 1
Time:  27.62s, ---- Loss: 0.2100, Acc.: 0.9417, Val. Loss: 0.4217, Val. Acc.: 0.9412

Saving at models/2_small/hist.png
