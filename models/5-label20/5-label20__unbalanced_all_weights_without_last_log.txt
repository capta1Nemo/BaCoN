
 -------- Parameters:
bayesian True
test_mode False
n_test_idx 2
seed 1312
fine_tune True
one_vs_all True
c_0 ['lcdm']
c_1 ['dgp', 'fR', 'rand', 'wcdm']
dataset_balanced False
include_last False
log_path models/5-label20/5-label20_log.txt
restore False
fname 5-label20
model_name custom
my_path None
DIR data/train_data/
TEST_DIR data/test_data/
models_dir models/
save_ckpt True
out_path_overwrite False
im_depth 500
im_width 1
im_channels 4
swap_axes True
sort_labels True
normalization stdcosmo
sample_pace 4
k_max 2.5
i_max None
add_noise True
n_noisy_samples 10
add_shot True
add_sys True
sigma_sys 5.0
z_bins [0, 1, 2, 3]
n_dense 1
filters [8, 16, 32]
kernel_sizes [10, 5, 2]
strides [2, 2, 1]
pool_sizes [2, 2, 0]
strides_pooling [2, 1, 0]
add_FT_dense False
trainable True
unfreeze False
lr 0.01
drop 0.5
n_epochs 10
val_size 0.15
test_size 0.0
batch_size 2500
patience 100
GPU True
decay 0.95
BatchNorm True
group_lab_dict {'dgp': 'non_lcdm', 'fR': 'non_lcdm', 'rand': 'non_lcdm', 'wcdm': 'non_lcdm', 'lcdm': 'lcdm'}

------------ CREATING DATA GENERATORS ------------
labels : ['lcdm', 'non_lcdm']
Labels encoding: 
{'lcdm': 0, 'non_lcdm': 1}
n_labels : 2
dgp - 18475 training examples
fR - 18475 training examples
lcdm - 18475 training examples
rand - 18475 training examples
wcdm - 18475 training examples

N. of data files: 18475
get_all_indexes labels dict: {'lcdm': 0, 'non_lcdm': 1}
create_generators n_labels: 2
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 15704
N of files in validation set: 2771
N of files in test set: 0
Check - total: 18475
--create_generators, train indexes
batch_size: 2500
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 15700
--create_generators, validation indexes
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 2750
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 15700, 2500, 5, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: True
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 2
list_IDs length: 15700
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 314
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 314

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: True
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 2
list_IDs length: 2750
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 55
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 55

------------ CREATING ORIGINAL DATA GENERATORS FOR CHECK------------
labels : ['dgp', 'fR', 'lcdm', 'rand', 'wcdm']
Labels encoding: 
{'dgp': 0, 'fR': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
n_labels : 5
dgp - 18475 training examples
fR - 18475 training examples
lcdm - 18475 training examples
rand - 18475 training examples
wcdm - 18475 training examples

N. of data files: 18475
get_all_indexes labels dict: {'dgp': 0, 'fR': 1, 'lcdm': 2, 'rand': 3, 'wcdm': 4}
create_generators n_labels: 5
create_generators n_labels_eff: 5
create_generators len_c1: 1
Check for no duplicates in test: (0=ok):
0.0
Check for no duplicates in val: (0=ok):
0
N of files in training set: 15704
N of files in validation set: 2771
N of files in test set: 0
Check - total: 18475
--create_generators, train indexes
batch_size: 2500
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Train index length: 15700
--create_generators, validation indexes
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
- Cut sample
bs: 2500
N_labels: 5
N_noise: 10
len_c1: 1
Val index length: 2750
len(train_index_1), batch_size, n_labels_eff, n_noisy_samples = 15700, 2500, 5, 10

--DataGenerator Train
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
list_IDs length: 15700
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 314
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 314

--DataGenerator Validation
Data Generator Initialization
Using z bins [0, 1, 2, 3]
Specified k_max is 2.5
Corresponding i_max is 100
Closest k to k_max is 2.539859
New data dim: (100, 1)
Final i_max used is 100
one_vs_all: False
dataset_balanced: False
base_case_dataset: True
N. classes: 5
N. n_classes in output: 5
list_IDs length: 2750
n_indexes (n of file IDs read for each batch): 50
batch size: 2500
n_batches : 55
For each batch we read 50 file IDs
For each file ID we have 5 labels
For each ID, label we have 10 realizations of noise
In total, for each batch we have 2500 training examples
Input batch size: 2500
N of batches to cover all file IDs: 55
------------ DONE ------------

------------ BUILDING MODEL ------------
Input shape (100, 4)
using 1D layers and 4 channels
Expected output dimension of layer conv1d_flipout: 46.0
Expected output dimension of layer max_pooling1d: 23.0
Expected output dimension of layer conv1d_flipout_1: 10.0
Expected output dimension of layer max_pooling1d_1: 9.0
Expected output dimension of layer conv1d_flipout_2: 8.0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 conv1d_flipout (Conv1DFlipo  (None, 46, 8)            648       
 ut)                                                             
                                                                 
 max_pooling1d (MaxPooling1D  (None, 23, 8)            0         
 )                                                               
                                                                 
 batch_normalization (BatchN  (None, 23, 8)            32        
 ormalization)                                                   
                                                                 
 conv1d_flipout_1 (Conv1DFli  (None, 10, 16)           1296      
 pout)                                                           
                                                                 
 max_pooling1d_1 (MaxPooling  (None, 9, 16)            0         
 1D)                                                             
                                                                 
 batch_normalization_1 (Batc  (None, 9, 16)            64        
 hNormalization)                                                 
                                                                 
 conv1d_flipout_2 (Conv1DFli  (None, 8, 32)            2080      
 pout)                                                           
                                                                 
 batch_normalization_2 (Batc  (None, 8, 32)            128       
 hNormalization)                                                 
                                                                 
 global_average_pooling1d (G  (None, 32)               0         
 lobalAveragePooling1D)                                          
                                                                 
 dense_flipout (DenseFlipout  (None, 32)               2080      
 )                                                               
                                                                 
 batch_normalization_3 (Batc  (None, 32)               128       
 hNormalization)                                                 
                                                                 
 dense_flipout_1 (DenseFlipo  (None, 5)                325       
 ut)                                                             
                                                                 
=================================================================
Total params: 6,781
Trainable params: 6,605
Non-trainable params: 176
_________________________________________________________________
None
Loss before loading weights/ 1.6199311

Loading ckpt from models/5-label20/tf_ckpts/
Loading ckpt models/5-label20/tf_ckpts/ckpt-9
Last learning rate was <keras.optimizer_v2.learning_rate_schedule.ExponentialDecay object at 0x7fa8b44eef70>
Learning rate set to 0.01
Loss after loading weights/ 0.21113859

Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 100, 4)]          0         
                                                                 
 input_1 (InputLayer)        multiple                  0         
                                                                 
 conv1d_flipout (Conv1DFlipo  (None, 46, 8)            648       
 ut)                                                             
                                                                 
 max_pooling1d (MaxPooling1D  (None, 23, 8)            0         
 )                                                               
                                                                 
 batch_normalization (BatchN  (None, 23, 8)            32        
 ormalization)                                                   
                                                                 
 conv1d_flipout_1 (Conv1DFli  (None, 10, 16)           1296      
 pout)                                                           
                                                                 
 max_pooling1d_1 (MaxPooling  (None, 9, 16)            0         
 1D)                                                             
                                                                 
 batch_normalization_1 (Batc  (None, 9, 16)            64        
 hNormalization)                                                 
                                                                 
 conv1d_flipout_2 (Conv1DFli  (None, 8, 32)            2080      
 pout)                                                           
                                                                 
 batch_normalization_2 (Batc  (None, 8, 32)            128       
 hNormalization)                                                 
                                                                 
 global_average_pooling1d (G  (None, 32)               0         
 lobalAveragePooling1D)                                          
                                                                 
 dense_flipout (DenseFlipout  (None, 32)               2080      
 )                                                               
                                                                 
 batch_normalization_3 (Batc  (None, 32)               128       
 hNormalization)                                                 
                                                                 
 dense_flipout_2 (DenseFlipo  (None, 2)                130       
 ut)                                                             
                                                                 
=================================================================
Total params: 6,586
Trainable params: 6,410
Non-trainable params: 176
_________________________________________________________________
None
GPU device not found ! Device: 
------------ TRAINING ------------

Features shape: (2500, 100, 4)
Labels shape: (2500, 2)
Initializing checkpoint from scratch.
Epoch 0
Validation loss decreased. Saved checkpoint for step 1: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-1
Time:  260.41s, ---- Loss: 0.1207, Acc.: 0.9412, Val. Loss: 0.1845, Val. Acc.: 0.9461

Epoch 1
Validation loss decreased. Saved checkpoint for step 2: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-2
Time:  248.80s, ---- Loss: 0.1246, Acc.: 0.9504, Val. Loss: 0.1791, Val. Acc.: 0.9467

Epoch 2
Validation loss decreased. Saved checkpoint for step 3: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-3
Time:  254.96s, ---- Loss: 0.1190, Acc.: 0.9514, Val. Loss: 0.1750, Val. Acc.: 0.9466

Epoch 3
Validation loss decreased. Saved checkpoint for step 4: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-4
Time:  248.02s, ---- Loss: 0.1204, Acc.: 0.9519, Val. Loss: 0.1693, Val. Acc.: 0.9488

Epoch 4
Validation loss decreased. Saved checkpoint for step 5: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-5
Time:  245.55s, ---- Loss: 0.1167, Acc.: 0.9524, Val. Loss: 0.1682, Val. Acc.: 0.9488

Epoch 5
Validation loss decreased. Saved checkpoint for step 6: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-6
Time:  268.58s, ---- Loss: 0.1216, Acc.: 0.9525, Val. Loss: 0.1671, Val. Acc.: 0.9487

Epoch 6
Validation loss decreased. Saved checkpoint for step 7: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-7
Time:  250.18s, ---- Loss: 0.1192, Acc.: 0.9525, Val. Loss: 0.1658, Val. Acc.: 0.9486

Epoch 7
Validation loss decreased. Saved checkpoint for step 8: models/5-label20/tf_ckpts_fine_tuning_unbalanced_all_weights_without_last/ckpt_fine_tuning_unbalanced_all_weights_without_last-8
Time:  249.83s, ---- Loss: 0.1194, Acc.: 0.9533, Val. Loss: 0.1613, Val. Acc.: 0.9510

Epoch 8
Loss did not decrease. Count = 1
Time:  254.89s, ---- Loss: 0.1184, Acc.: 0.9538, Val. Loss: 0.1618, Val. Acc.: 0.9498

Epoch 9
Loss did not decrease. Count = 2
Time:  257.78s, ---- Loss: 0.1133, Acc.: 0.9540, Val. Loss: 0.1616, Val. Acc.: 0.9502

Saving at models/5-label20/hist_fine_tuning_unbalanced_all_weights_without_last.png
